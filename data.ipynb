{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom shutil import copyfile\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\n# Define categories and paths\nCATEGORIES = [\"Atelectasis\", \"Infiltration\", \"Effusion\", \"Nodule\", \"Mass\"]\nIMAGE_FOLDERS = [f\"/kaggle/input/data/images_{i:03d}/images\" for i in range(1, 13)]\nCSV_FILE = \"/kaggle/input/data/Data_Entry_2017.csv\"\nOUTPUT_PATH = \"./output/\"\nLOG_FILE = \"./file_distribution_log.csv\"\n\n# Maximum counts\nMAX_TRAIN = 1100\nMAX_VAL = 200\n\n# Create folders for each category\ndef create_folders(base_path, categories):\n    for category in categories:\n        os.makedirs(os.path.join(base_path, \"train\", category), exist_ok=True)\n        os.makedirs(os.path.join(base_path, \"val\", category), exist_ok=True)\n\n# Locate an image in the distributed image folders\ndef locate_image(image_name):\n    for folder in IMAGE_FOLDERS:\n        potential_path = os.path.join(folder, image_name)\n        if os.path.exists(potential_path):\n            return potential_path\n    return None\n\n# Logging function\ndef log_file(image_name, label, folder_type):\n    with open(LOG_FILE, 'a') as f:\n        f.write(f\"{image_name},{label},{folder_type}\\n\")\n\n# Function to distribute files with count limits, ensuring no duplicates by base name\ndef distribute_files(row, train_folder, val_folder, train_count, val_count, used_base_names):\n    image_name = row['Image Index']\n    label = row['Labels'][0]  # Since we only consider single-labeled images\n\n    # Extract base name by removing suffix like _000, _001, etc.\n    base_name = image_name.rsplit('_', 1)[0]\n\n    # If the base name has already been used for this category, skip it\n    if base_name in used_base_names[label]:\n        return\n\n    source = locate_image(image_name)\n\n    if source:\n        if row['is_train']:\n            if train_count[label] < MAX_TRAIN:\n                destination = os.path.join(train_folder, label, image_name)\n                copyfile(source, destination)\n                train_count[label] += 1\n                used_base_names[label].add(base_name)  # Mark the base name as used\n                log_file(image_name, label, \"train\")\n        else:\n            if val_count[label] < MAX_VAL:\n                destination = os.path.join(val_folder, label, image_name)\n                copyfile(source, destination)\n                val_count[label] += 1\n                log_file(image_name, label, \"val\")\n\n# Load the CSV file\ndata = pd.read_csv(CSV_FILE)\n\n# Filter relevant rows and columns\ndata['Labels'] = data['Finding Labels'].apply(lambda x: x.split('|'))\ndata = data[data['Labels'].map(lambda labels: len(labels) == 1 and labels[0] in CATEGORIES)]\n\n# Train-validation split\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\ntrain_data['is_train'] = True\nval_data['is_train'] = False\nsplit_data = pd.concat([train_data, val_data])\n\n# Create output folders\ncreate_folders(OUTPUT_PATH, CATEGORIES)\n\n# Initialize log file\nwith open(LOG_FILE, 'w') as f:\n    f.write(\"Image Name,Category,Folder Type\\n\")\n\n# Initialize counters for each category\ntrain_count = defaultdict(int)\nval_count = defaultdict(int)\n\n# Dictionary to track base image names to avoid duplicates\nused_base_names = defaultdict(set)\n\n# Distribute files\nsplit_data.apply(lambda row: distribute_files(row, os.path.join(OUTPUT_PATH, \"train\"), os.path.join(OUTPUT_PATH, \"val\"), train_count, val_count, used_base_names), axis=1)\n\nprint(f\"Dataset reorganization completed! File distribution logged in {LOG_FILE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-01T14:44:22.973883Z","iopub.execute_input":"2024-12-01T14:44:22.974231Z","iopub.status.idle":"2024-12-01T14:44:56.356820Z","shell.execute_reply.started":"2024-12-01T14:44:22.974205Z","shell.execute_reply":"2024-12-01T14:44:56.355889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to output folder and ZIP file\noutput_folder = \"./output\"\nzip_file = \"./output.zip\"\n\n# Compress the folder\nshutil.make_archive(output_folder, 'zip', output_folder)\n\nprint(f\"Zipped output saved as {zip_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T14:46:16.128603Z","iopub.execute_input":"2024-12-01T14:46:16.128891Z","iopub.status.idle":"2024-12-01T14:47:36.439566Z","shell.execute_reply.started":"2024-12-01T14:46:16.128865Z","shell.execute_reply":"2024-12-01T14:47:36.438665Z"}},"outputs":[],"execution_count":null}]}